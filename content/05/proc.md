Data processing and analysis
=========================================================

```{admonition} Issue
Researchers typically execute a set of signal pre-processing steps prior to advanced data analysis, to, for instance, identify and remove noise, align data spatially and temporally, segment spatio-temporal regions of interest, identify patterns and latent signal structures (e.g., clustering), integrate the information from several modalities, introduce prior knowledge about the device or the physiology of the specimen, etc. The combination of the operations that take the unprocessed data as the input, prepare the data for analysis, and finally, perform advanced analysis, comprise a full analysis pipeline or workflow. In implementing such analysis workflows, software has emerged as a critical research instrument greatly relevant to ensure the reproducibility of studies.
```

```{admonition} What do we provide
TBE.
```

```{figure} ../figures/fig4.png
---
name: fig4
---
Data processing and analysis [^footnote4].
```

:::{dropdown} {fa}`microscope` Software as a research instrument
:title: bg-ch5 font-weight-bold
:animate: fade-in
The digital nature of neuroimaging data along with the large, and constantly increasing, net amounts of daily acquired data, place software as a central instrument of the neuroimaging research workflow. As a result, many toolboxes containing utilities ranging from early steps of preprocessing to statistical analysis and visualization of results have emerged, and some have largely shaped the software development in the field, e.g., AFNI (Cox 1996; Cox and Hyde 1997), FSL (Jenkinson et al. 2012), SPM (Penny et al. 2011; Litvak et al. 2011; Flandin and Friston 2008), FreeSurfer (Dale, Fischl, and Sereno 1999; Dale and Sereno 1993), Brainstorm (Tadel et al. 2011, 2019), EEGLAB (Delorme and Makeig 2004; Delorme et al. 2021), MNE-Python (Gramfort et al. 2013, 2014), FieldTrip (Oostenveld et al. 2011) (see Table S1). More recently, some software packages have been developed to cover additional aspects of the neuroimaging workflow. For instance, nibabel (Brett et al. 2020) to read and write images in many formats, the Advanced Normalization Tools (ANTs) for image registration and segmentation, or Nilearn (Abraham et al. 2014) for statistical analysis and visualization. Workflow engines conveniently connect between the building blocks and determine how the steps are executed in the computational environment. Solutions range from general-purpose scripting (e.g., Bash or Python) to neuroimaging-specific libraries (e.g., NiPype; Gorgolewski et al. 2011). Researchers have all these tools (and others) at their disposal to “mix-and-match” in their workflow. Therefore, ensuring the proper development and operation of the software engine is critical to ensure the reproducibility of results (Tustison et al. 2013).

Relatedly, the variety of software implementations is an additional motive of concern. As remarked by Carp (2012a a, [b] 2012 b) based on the analysis of thousands of fMRI pipelines, analytical flexibility in combination with incomplete reporting precludes the reproducibility of the results. A recent comprehensive investigation, the Neuroimaging Analysis Replication and Prediction Study (NARPS; Botvinik-Nezer et al. 2020) found that when 70 different teams were asked to analyze the same fMRI data to test the same hypotheses, each team chose a distinct pipeline and results were highly variable. Other studies suggest similar problems in EEG (Šoškić et al. 2021; Clayson et al. 2021), PET (Nørgaard et al. 2020) and diffusion MRI (Schilling et al. 2021).

There are two crucial aspects of the high analytical variability and its effect on results in neuroimaging. First, when high analytical variability (that potentially affects results) is combined with partial reporting or with incentives to find significant effects, it can alarmingly undermine the reliability and reproducibility of results. Second, even in the apparently ideal scenario in which the researcher performs a single pre-registered valid analysis and reports it fully and transparently, it is still likely that the results are not robust to arbitrary analytical choices. Therefore, new tools are needed to allow researchers to perform a “multiverse analysis” (section 5.4), where multiple data workflows are used on the same dataset and all the results are reported and their agreement or convergence discussed. Community-led efforts to develop high-quality “gold standard” data workflows may also reduce researchers’ degrees of freedom as well as accelerate data analysis, although different pipelines may be optimal for different research questions and data.

Nevertheless, neuroimaging researchers frequently encounter gaps that readily available toolboxes do not cover. These gaps, amongst a number of other reasons (e.g., deploying a data workflow on a high-performance computer), pushes researchers into creating their own software implementations. However, most neuroimaging researchers are not formally trained in related fields of computer science, data science, or software engineering, and formal software development practices are often not included in undergraduate or graduate level neuroimaging training. This mismatch often results in undocumented, hard to maintain, and disorganized code; largely as a consequence of unawareness of software development practices. It also increases the likelihood of undetected errors that may remain even after running tests on the code.

The first and foremost strategy available to maximize the transparency of research methods is openly sharing the code with the minimal restrictions possible (see section 6.2; Barnes 2010; Gorgolewski and Poldrack 2016). Complementarily, version control systems, such as Git (Blischak, Davenport, and Wilson 2016, see Table S1), are the most basic and effective tool to track how software is developed, and to collaboratively produce code. Beyond making the code available to others, software tools can implement further transparency strategies by thoroughly documenting their tools and by supporting implementations with scientific publications (Barnes 2010; Gorgolewski and Poldrack 2016).
:::


:::{dropdown} {fa}`microchip` Standardizing preprocessing and workflows
:title: bg-ch5 font-weight-bold
:animate: fade-in
Although the diversity in methodological alternatives has been key to extracting scientific insights from neuroimaging data, appropriately combining heterogeneous tools into complete workflows requires substantial expertise. Traditionally, researchers used default workflows distributed along with individual software packages, or alternatively, individual laboratories have developed in-house analysis workflows that resulted in highly specialized pipelines. Such pipelines are often not thoroughly validated and difficult to reuse due to lack of documentation or accessibility to outside labs. In response, several community-led efforts have spearheaded the development of robust, standardized workflows.

An early effort towards workflow standardization was the Configurable Pipeline for the Analysis of Connectomes (C-PAC; Craddock et al. 2013), which is a “nose-to-tail” preprocessing and analysis pipeline for resting state fMRI. C-PAC offers a comprehensive configuration file, editable directly with a text editor or through C-PAC’s graphical user-interface, prescribing all the tools and parameters to be executed, and thereby making strides towards keeping methodological decisions closely traced. Similarly, large-scale acquisition initiatives released workflows tailored for their official imaging protocols (e.g., the HCP Pipelines (Glasser et al. 2013) and the UK Biobank (Alfaro-Almagro et al. 2016)).
Conversely, fMRIPrep (Esteban, Markiewicz, et al. 2019) proposed the alternative approach of restricting the pipeline goals to the preprocessing step, while accepting the maximum diversity possible of the input data (i.e., not tailored to a particular experimental design or analysis-agnosticity). This approach has recently been proposed for additional modalities (e.g., dMRI, ASL, PET) and population/species of interest (e.g., fMRIPrep-rodents, fMRIPrep-infants) under a common framework called NiPreps (NeuroImaging PREProcessing toolS). NiPreps is a community-led endeavor with the goal of ensuring the generalization of the building blocks of preprocessing across modalities (e.g., the alignment of fMRI and dMRI with the same participant / animal’s anatomical image) and specimens (e.g., using the same brain extraction from anatomical data using the same algorithm and implementation on both human adults and rodents). Similar standardization efforts are starting to be adopted for EEG (Desjardins et al. 2021) and MEG (e.g., MNE-BIDS pipeline; Jas et al. 2018). Further examples of standardized workflows are found in Table S1.

An additional and relevant premise of standardized workflows is transparency — tools must be transparent not only in their implementation, but also in their reporting. For example, fMRIPrep produces visual reports with the double goal of assessing the quality of results, and also providing the researcher with a resource to comprehensively understand every step of the workflow. In addition, the report includes a text description which comprehensively describes each major step in the pipeline, including the exact software version and principle citation. This text, referred to as the “citation boilerplate”, is released under a public domain license, and therefore can be included verbatim in researcher’s manuscripts, facilitating accurate reporting and proper referencing of academic software. A final relevant aspect towards transparency is the comprehensive documentation of pipelines.

In most cases, standardized workflows preprocess datasets in a fully automated manner, taking a BIDS dataset as input and outputting data that is ready for subsequent analysis with little manual intervention. Importantly, such workflows are typically designed to be as robust as possible to diverse input data (e.g., with varying parameters or sampling distant populations), a challenge that is facilitated by data standardization (i.e., BIDS). Additionally, workflows must be portable, enabling users to execute them in a wide variety of environments. A key technology in this endeavor is containers—such as Docker and Apptainer/Singularity—which facilitate packaging specific versions of heterogeneous dependencies while ensuring cross-platform compatibility (e.g., high-performance computing clusters, desktop, or cloud services). The BIDS apps framework (section 4.1) leverages containers by standardizing input parameters to make it trivially easy to execute a wide variety of standardized workflows on BIDS datasets. An example of a higher-level combination of workflows is found in Esteban et al. (2020), which describes an MRI research protocol using MRIQC and fMRIPrep. Finally, recent efforts to standardize the outputs of workflows (BIDS Derivatives), further enhances the interoperability of workflows, by ensuring their outputs are compatible with subsequent analysis.
:::

:::{dropdown} {fa}`calculator` Statistical modeling and advanced analysis
:title: bg-ch5 font-weight-bold
:animate: fade-in
Analysis of neuroimaging data is particularly heterogeneous and prone to excessive analytical flexibility and underspecified reporting (Carp 2012a a, [b] 2012 b). Whereas preprocessing is ideally performed once per dataset, there is often a large number of types of analyses that may be used with the preprocessed data. In MRI and fNIRS, for example, analyses range from multi-stage general linear models (GLMs), multivariable decoding analyses, to anatomical and functional connectivity, and more. In PET, analyses consist of region-wise averaging, although voxel-wise approaches are gaining popularity, followed by kinetic modeling and subsequent statistical analyses, which can be GLM or more advanced, such as latent variable models. In MEG and EEG, the broad variety includes analyses such as evoked response potentials, power spectral density, source reconstructions, time-frequency, connectivity, advanced statistics and more. Each type of analysis also has a wide variety of subtypes, parameters, and statistical models that can be specified, and the form of that specification varies across the dozens of analysis packages that implement each type of analysis.

Data analysis reporting may be made more transparent by sharing code that relies on open-source software. A prime example is SPM (Flandin and Friston 2008), which has been open source since its inception in 1991. Additional widely used open-source tools for data analysis are FSL and AFNI for MRI, and some examples of reproducible pipelines for MEG and EEG developed based on each of the following software: EEGLAB (Pernet, Martinez-Cancino, et al. 2020) Fieldtrip (Andersen 2018b; M. Meyer et al. 2021; Popov, Oostenveld, and Schoffelen 2018), Brainstorm (Niso et al. 2019; Tadel et al. 2019), SPM (Henson et al. 2019) and MNE-Python (Andersen 2018a; van Vliet et al. 2018; Jas et al. 2018) (see Niso et al. (2022) for a detailed review on main EEG and MEG open toolboxes and reproducible pipelines). Reproducibility is also improved when relying on modular and well-documented software such as Nilearn, which offers versatile methods to perform advanced analyses of fMRI data, from GLM to connectomic and machine learning (Abraham et al., 2014). Ideally, a single analysis script is made creating a report, from signal extraction, data analysis, and reproducing all figures.

An additional challenge for the reproducibility of analysis workflows is the representation of statistical models across distinct implementations of analysis software. For example, GLM approaches to analyze fMRI time series are prevalent and supported by all of the major statistical packages (e.g., AFNI, SPM, FSL, Nilearn). However, specifying equivalent models across packages is non-trivial and requires time consuming package specific model specification (Pernet 2014), which obfuscates details of the statistical model, exacerbates variability across pipelines, and makes it difficult to perform multiverse analyses (see section 5.4). The BIDS Stats Model (BIDS-SM, see Table S1) specification has been proposed as a implementation-independent representation of fMRI GLM models; BIDS-SM describes the inputs, steps, and specification details of GLM-type analyses, and encodes them in a machine readable JSON format. The PyBIDS library provides tooling to facilitate reading BIDS-SM, and FitLins (Markiewicz, De La Vega, et al. 2021) is a reference workflow that fits BIDS-SM using AFNI or Nilearn. The transformative potential of BIDS-SM is showcased by Neuroscout (de la Vega et al. 2022), a turnkey platform for fast and flexible neuroimaging analysis. Neuroscout provides a user-friendly web application for creating BIDS-SM on a curated set of public neuroimaging datasets, and leverages FitLins to fit statistical models in a fully reproducible and portable workflow. By standardizing the entire process of statistical modeling, users can formally specify a hypothesis and produce statistical results in a matter of minutes, while simultaneously ensuring a fully reproducible and transparent analysis that can be readily disseminated to the scientific community.
:::


:::{dropdown} {fa}`globe` Multiverse analysis
:title: bg-ch5 font-weight-bold
:animate: fade-in
The variety of data workflows reflects the enormous interest and the need for novel software instruments, but it also poses an important risk to reproducibility. The multitude of possible combinations of methods and parameters in each of the analysis steps creates an extremely large number of combinations to select from. This problem is often referred to as “researcher degrees of freedom” or “the garden of forking paths” (Gelman and Loken 2013). Importantly, analytical choices affect results. This has been shown for preprocessing of fMRI data already back in 2004 (Strother et al. 2004). More work in this direction followed in 2012 (Churchill, Oder, et al. 2012; Churchill, Yourganov, et al. 2012). While this work focused mainly on the aspect of tailoring preprocessing to e.g. maximize predictive models, recent efforts in fMRI (task fMRI: (Botvinik-Nezer et al. 2020; Carp 2012a); preprocessing of resting-state fMRI: (Li et al. 2021)) and PET (specifically for preprocessing: Nørgaard et al. 2020) focused more on the variability of outcomes in general when analysis pipelines were varied. In addition, recent studies showed high variability in diffusion-based tractography dissection (Schilling et al. 2021) and event-related potentials in EEG preprocessing (Šoškić et al. 2021; Clayson et al. 2021). Another large-scale attempt to estimate the analytical variability for EEG,  EEGManyPipelines (see Table S1), is currently ongoing.

The converging findings of these studies across modalities suggest that it is important to test the robustness of reported results to specific analytical choices. One proposed solution to tackle the analytical variability, where many different analytical approaches are compared, is multiverse analysis (Hall et al. 2022). There are two broad types of multiverse tools. In a “numerical instabilities” approach, different setups and numerical errors or uncertainties in computational tools are evaluated, analyses are rerun several times, and variability, robustness, and “mean answer” are estimated (Gregory Kiar et al. 2020). One tool of this type that is being developed is “Fuzzy” (Greg Kiar et al. 2021). Alternatively, in a “classic multiverse analysis”, multiple pipelines are used with the same data and the results are compared across pipelines. Such an analysis could be conducted by a single or by multiple researchers (Aczel et al. 2021). Although multiverse analysis was suggested before in other fields (Steegen et al. 2016; Simonsohn, Simmons, and Nelson 2015; Patel, Burford, and Ioannidis 2015), there are not yet mature “classic multiverse analysis” tools for high-dimensional data like in neuroimaging. Explorable Multiverse Analyses is an R-tool that allows the readers to explore different statistical approaches in a paper (Dragicevic et al. 2019). Other tools, such as the Python-based Boba (Liu et al. 2021), aim to facilitate multiverse analyses by allowing users to specify the shared and the varying parts of the code only once and by providing useful visualizations of the pipelines and results. However, these tools currently fit simpler analyses and datasets compared to the ones common in neuroimaging.

In neuroimaging, recent progress has been made in creating infrastructure for multiverse analysis in fMRI, based on the C-PAC tool (see section 5.2; Li et al. 2021). Ongoing efforts to formalize machine-readable standards for statistical models (BIDS-SM) and pipelines to estimate them (e.g., FitLins; Markiewicz, De La Vega, et al. 2021), and their integration with datasets using platforms such as brainlife.io (Avesani et al., 2019), could facilitate the development of multiverse tools. In order to make sense of a multiverse analysis, one needs methods to test for convergence across results of diverse analysis pipelines with the same data. Such a method for fMRI image-based meta-analysis was recently used in NARPS (Botvinik-Nezer et al. 2020) as well as in subsequent projects (Bowring, Nichols, and Maumet 2021). Another simple statistical approach to a multiverse analysis was presented with PET data (Nørgaard, Ozenne, et al. 2019), although it  lacks statistical power, due to the use of a very conservative statistic. A different approach is to use active learning to approximate the whole multiverse space (Dafflon et al. 2020). Moreover, Boos et al. (2021) provided an online application to explore the effects of the choice of parameters on the results (data-driven auditory encoding, see Table S1). Progress is still needed until such tools are mature enough to allow scalable multiverse analysis in neuroimaging.
:::

[^footnote4]: Sources: Icons from the Noun Project: Software by Adrien Coquet; Workflow by D. Sahua; Statistics by Creative Stall; Chaos Sigil by Avana Vana; Logos: used with permission by the copyright holders.

:::{dropdown} References on this page
```{bibliography}
:filter: docname in docnames
:labelprefix: E
```
:::
